{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"POSTagger.ipynb","provenance":[],"collapsed_sections":["81R2Qp-Ix9G0","Mz8HGMzeGuUl","STpZk5qBG_Pv"],"authorship_tag":"ABX9TyO1tBCmXNWdX8A4M4Yy0ASf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DFbv5_zVGCTs"},"source":["# **POS Tagging with Penn Treebank dataset**"]},{"cell_type":"code","metadata":{"id":"cDddHDpmm3wn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yoAdDT6xI01","executionInfo":{"status":"ok","timestamp":1627460447237,"user_tz":-120,"elapsed":1302,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}},"outputId":"4516e8db-67d9-45c7-8ea7-3c04045893f7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"81R2Qp-Ix9G0"},"source":["## 01. Penn Treebank dataset preparation\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"BWGu9temGM9b","executionInfo":{"status":"ok","timestamp":1627459989466,"user_tz":-120,"elapsed":3645,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}}},"source":["# Penn Treebank English Dataset Preparation\n","import pickle\n","import gzip\n","\n","class PennTreeBankDataset():\n","\n","    def load_file(self, filename:str): # given file names, returns each line\n","        file = open(filename)\n","        roh_daten = file.readlines()\n","        return roh_daten\n","\n","    def preprocessing(self, file:str):\n","        #step 1: fetch file contents\n","        raw_dataset = self.load_file(file)\n","\n","        #step 2: form sentences and corresponding POS tags:\n","        all_samples, all_labels = [], []\n","        for sample in raw_dataset:\n","\n","            sample = sample.replace('\\n','')           \n","            sentence_dirty = list(filter(None,sample.split(')'))) # spliiting on closing brackets and viewing a subset of the result \n","            sentence_clean, tags_clean = [],[]\n","            \n","            for word in sentence_dirty:\n","                word = word.replace(\"(\",\"\")\n","                sentence_clean.append(word.split(' ')[-1])\n","                tags_clean.append(word.split(' ')[-2]) if len(word) > 2 else ''\n","            \n","            if len(sentence_clean) != len(tags_clean):\n","                print(\"Mismatch in no. of tokens in the line!\")\n","                break\n","            \n","            # add these two to the big list\n","            all_samples.append(sentence_clean) \n","            all_labels.append(tags_clean)\n","\n","        if len(all_samples) != len(all_labels):\n","            print(f\"Total no. of samples: {len(all_samples)} \\nTotal no. of POS tags: {len(all_labels)}\")\n","            print(\"Mismatch in no. of lines\")\n","            exit\n","\n","        return list(zip(all_samples,all_labels))\n","\n","    def export_files(self): # one time to create datasets\n","\n","        list_of_files = [\"PennTreeBankTrain.pklz\",\"PennTreeBankTest.pklz\",\"PennTreeBankValid.pklz\"]\n","        list_of_files_tags = [\"PennTreeBankTrainPOS.pkl\",\"PennTreeBankTestPOS.pkl\",\"PennTreeBankValidPOS.pkl\"]\n","        list_of_datasets = [\"02-21.10way.clean.txt\",\"23.auto.clean.txt\",\"22.auto.clean.txt\"]\n","\n","        for final,pos,raw in zip(list_of_files,list_of_files_tags,list_of_datasets):\n","            # print(f\"Processing: {raw} => {final} \\t {pos}\")\n","            dataset = self.preprocessing(file=\"/content/\"+str(raw))\n","            # print(len(dataset))\n","            with gzip.open('/content/' +str(final), 'wb') as f:\n","                pickle.dump(dataset, f)\n","                f.close()\n","\n","ds = PennTreeBankDataset()\n","a = ds.export_files()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mz8HGMzeGuUl"},"source":["## 02. Prepare Dictionary:"]},{"cell_type":"code","metadata":{"id":"mhJy5weOGx7O","executionInfo":{"status":"ok","timestamp":1627460503022,"user_tz":-120,"elapsed":245,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}}},"source":["\n","\n","class PennTreeBankDictionary():\n","\n","    def load_corpus(self):\n","        ds1 = PennTreeBankDataset()\n","\n","        print(\"preparing train/test/valid datasets\")\n","        valid_ds = ds1.preprocessing(file=\"/content/22.auto.clean.txt\")\n","        test_ds = ds1.preprocessing(file=\"/content/23.auto.clean.txt\")\n","        train_ds = ds1.preprocessing(file=\"/content/02-21.10way.clean.txt\")\n","        complete_ds = valid_ds + test_ds + train_ds\n","        print(\"done\")\n","\n","        return complete_ds\n","\n","    def tokens_and_tags(self):\n","\n","        #Step 1: fetch dataset\n","        dataset = self.load_corpus()\n","        tokens, tags = [], []\n","\n","        #Step 2: split sentences and pos tags to two separate list\n","        for sample in dataset:\n","            tokens.append(sample[0])\n","            tags.append(sample[1])\n","\n","        #Step 3: list of lists to a single flat list and take unique words and pos tags\n","        all_tokens = [item for sublist in tokens for item in sublist]\n","        all_tokens = list(set(all_tokens))\n","        all_tags = [item for sublist in tags for item in sublist]\n","        all_tags = list(set(all_tags))\n","\n","        return all_tokens, all_tags\n","\n","    def vocabulary(self):\n","        \n","        print(\"preparing look-up dictionaries\")\n","        words_in_corpus, pos_tags_in_corpus = self.tokens_and_tags()\n","        words_in_corpus.append('PADDING')\n","        pos_tags_in_corpus.append('PADDING')\n","        word_to_idx, pos_to_idx = {}, {}\n","        idx_to_word, idx_to_pos = {}, {} # for reverse look-up\n","\n","        for idx, word in enumerate(words_in_corpus):\n","            word_to_idx[word] = idx\n","            idx_to_word[idx] = word\n","\n","        for idx,pos in enumerate(pos_tags_in_corpus):\n","            pos_to_idx[pos] = idx\n","            idx_to_pos[idx] = pos\n","\n","        print(\"done!\")\n","        print(f\"Total words in the dictionary: {len(word_to_idx)} \\nTotal POS tags in the dictionary: {len(pos_to_idx)}\")\n","        return word_to_idx, idx_to_word, pos_to_idx, idx_to_pos\n","\n","# ex = PennTreeBankDictionary()\n","# a,b,c,d = ex.vocabulary()\n","# print(list(a.items())[:10])\n","# print(list(c.items())[:10])\n"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STpZk5qBG_Pv"},"source":["## 03. Fetch dataset"]},{"cell_type":"code","metadata":{"id":"CDpb0fnhHH9d","executionInfo":{"status":"ok","timestamp":1627460560085,"user_tz":-120,"elapsed":230,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}}},"source":["from typing import List, Tuple\n","# from prepareDictionary import PennTreeBankDictionary\n","ds = PennTreeBankDictionary()\n","# from prepareDataset import PennTreeBankDataset\n","import torch\n","import pickle, gzip\n","from torch.utils.data import Dataset, DataLoader\n","\n","class myDataset(Dataset):\n","\n","    def compose_dictionaries(self): \n","\n","        word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds.vocabulary()\n","        return word_to_idx, idx_to_word, pos_to_idx, idx_to_pos\n","\n","    def file_parser(self,filename) -> Tuple[List,List]:\n","\n","        filepath = \"/content/\"+filename+'.pklz'\n","        samples_with_labels = []\n","        file = gzip.open(filepath,'rb')\n","        samples_with_labels = pickle.load(file)\n","        \n","        return samples_with_labels\n","\n","    def file_tensor(self, sentences_and_tags) -> Tuple[List, List]:\n","\n","        def token_pipeline(x):\n","            if len(x) < 50:\n","                for i in range(0,50-len(x)):\n","                    x.append('PADDING')\n","            return [self.word_to_idx[tok] for tok in x]\n","\n","        def pos_pipeline(x):\n","            if len(x) < 50:\n","                for i in range(0,50-len(x)):\n","                    x.append('PADDING')\n","            return [self.pos_to_idx[pos] for pos in x]\n","\n","        sent_to_idx, tags_to_idx = [], []\n","        for sent_tag in sentences_and_tags:\n","            if len(sent_tag[0]) >50:\n","                continue\n","            sent_to_idx.append(torch.tensor(token_pipeline(sent_tag[0])))\n","            tags_to_idx.append(torch.tensor(pos_pipeline(sent_tag[1])))\n","\n","        return sent_to_idx, tags_to_idx\n","\n","    def __init__(self, raw_dataset=None):\n","\n","        print(\"STEP 01: Look-up Tables...\")\n","        self.word_to_idx, self.idx_to_word, self.pos_to_idx, self.idx_to_pos = self.compose_dictionaries()\n","        print(\"dictionaries ready!\")\n","\n","        print(\"STEP 02: Fetching the dataset...\")\n","        self.samples_with_labels = self.file_parser(raw_dataset)\n","        print(\"done!\")\n","\n","        print(\"STEP 03: Tokens and tags to Numbers...\")\n","        self.samples_to_idx, self.labels_to_idx = self.file_tensor(self.samples_with_labels)\n","\n","    def __len__(self):    \n","        return len(self.samples_to_idx)\n","\n","    def __getitem__(self, index):\n","        return self.samples_to_idx[index], self.labels_to_idx[index]\n","\n","# validation_dataset = DataLoader(dataset=myDataset(\"PennTreeBankValid\")\n","#                                         ,shuffle=False\n","#                                         ,batch_size=16)\n","\n"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"riAeVu68HQbS"},"source":["## 04. Neural Network Model"]},{"cell_type":"code","metadata":{"id":"tpQEKGCzHWbj","executionInfo":{"status":"ok","timestamp":1627462931152,"user_tz":-120,"elapsed":253,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}}},"source":["# from typing import final\n","from torch import nn\n","import torch\n","\n","\"\"\"RNN Many-to-many multi-class classification neural network model structure definition\"\"\"\n","\n","class RNNPOSTagger(nn.Module):\n","\n","    def __init__(self, \n","                embedding_dimension, \n","                vocabulary_size,\n","                hidden_dimension,\n","                num_of_layers,\n","                dropout,\n","                output_dimension\n","                ):\n","        super(RNNPOSTagger, self).__init__()\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n","                                    embedding_dim=embedding_dimension,\n","                                    padding_idx=45)\n","\n","        self.lstm = nn.LSTM(embedding_dimension,\n","                            hidden_dimension,\n","                            num_of_layers,\n","                            dropout=dropout,\n","                            batch_first=True)\n","                            # bidirectional=True)\n","\n","        self.fc = nn.Linear(hidden_dimension, output_dimension)\n","\n","        # self.activation_fn = nn.Tanh()\n","        self.activation_fn = nn.LogSoftmax(dim=1)\n","        \n","        # self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, sample):\n","\n","        # (1)- Embedding layer\n","        embedded = self.embedding(sample)\n","\n","        #-------------------------------------------------------------------------\n","\n","        #(2)- LSTM layer 1\n","        output, (hidden, cell) = self.lstm(embedded)       \n","\n","        #-------------------------------------------------------------------------\n","\n","        #concat the final forward and backward hidden state\n","        hidden = torch.cat((hidden[-1,:,:], hidden[0,:,:]), dim = 1)\n","\n","\n","        #(3)- LSTM to linear layer: Final set of tags\n","        dense_output = self.fc(output)\n","\n","        #activation function\n","        outputs=self.activation_fn(dense_output)\n"," \n","        return outputs\n","        # return dense_output"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SGi7mJXGHfoW"},"source":["## 05. Train Model"]},{"cell_type":"code","metadata":{"id":"k4UQIy6jHioe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627481386318,"user_tz":-120,"elapsed":295220,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}},"outputId":"61364955-6bb3-418b-9983-7dff64a17ecd"},"source":["from torch.utils.data import DataLoader\n","# from fetchDataset import myDataset\n","train_test_ds = myDataset\n","# from prepareDictionary import PennTreeBankDictionary\n","ds = PennTreeBankDictionary()\n","import time\n","import os\n","# from model import RNNPOSTagger\n","\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","\n","################################### 01. Train/Test dataset  ########################################\n","print(\"=\"*100)\n","print(\"01. Preparing train/test datasets:\")\n","\n","train_dataset = DataLoader(dataset=train_test_ds(\"PennTreeBankTrain\"), batch_size=8, shuffle=True)\n","test_dataset = DataLoader(dataset=train_test_ds(\"PennTreeBankTest\"), batch_size=8, shuffle=True)\n","# validation_dataset = DataLoader(dataset=myDataset(\"PennTreeBankValid\"),batch_size=16,shuffle=True)\n","print(\"datasets ready!\")\n","print(\"=\"*100)\n","\n","################################# 02.Model Parameters ####################################\n","print(\"02. Loading Model Parameters:\")\n","word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds.vocabulary()\n","\n","# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n","VOCAB_SIZE = len(word_to_idx)+1\n","EMBED_DIM = 100\n","HIDDEN_DIM = 64\n","NUM_LAYERS = 2\n","NUM_OF_CLASSES = len(pos_to_idx)\n","N_EPOCHS = 5\n","LEARNING_RATE = 0.025#0.1\n","BATCH_SIZE = 128#16\n","\n","print(f\"Size of vocabulary: {VOCAB_SIZE}\" + f\"\\tNumber of classes: {NUM_OF_CLASSES}\")\n","##################################### 03. NN Model  ########################################\n","\n","print(\"Step 02. builing the model...\")\n","model = RNNPOSTagger(embedding_dimension= EMBED_DIM,\n","                    vocabulary_size=VOCAB_SIZE,\n","                    hidden_dimension=HIDDEN_DIM,\n","                    num_of_layers=NUM_LAYERS,\n","                    dropout=0.1,\n","                    output_dimension=NUM_OF_CLASSES)\n","\n","print(\"Done! here is our model:\")\n","print(model)\n","print(\"=\"*100)\n","\n","############################# 04. Optimizer and Loss  ####################################\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","# criterion = nn.CrossEntropyLoss(ignore_index=45)\n","criterion = nn.NLLLoss(ignore_index=45)\n","\n","\n","#define metric\n","def train_accuracy(preds, y):\n","    predicted_labels_dirty = preds.permute(0,2,1)\n","    predicted_labels_final = torch.argmax(predicted_labels_dirty, dim=2).tolist()\n","    actual_labels_final = y.tolist()\n","    accuracy_of_all_lines = []\n","    for predicted, actual in zip(predicted_labels_final, actual_labels_final):\n","        counter = 0\n","        # print(predicted)\n","        # print(actual)\n","        for pred,act in zip(predicted,actual):\n","            if pred == act:\n","                counter = counter+1\n","        accuracy_of_this_line = counter/50\n","        accuracy_of_all_lines.append(accuracy_of_this_line)\n","    accuracy = sum(accuracy_of_all_lines)/len(predicted_labels_final)\n","\n","    return accuracy\n","\n","def training_accuracy(preds, y):\n","    \n","    predsx = preds.permute(0,2,1) #reshape\n","    predsx2 = torch.argmax(predsx, dim=2) #find POS index with max value for each token\n","\n","    for pred,act in zip(predsx2.tolist()[0],y.tolist()[0]):\n","        counter = 0\n","        if pred == act:\n","            counter = counter+1\n","        \n","    # correct = (predsx2 == y)\n","    # acc = correct.sum() / len(preds)\n","    acc = counter/len(preds)\n","    # print(type(acc))\n","\n","    return acc\n","    \n","#push to cuda if available\n","# model = model.to(device)\n","# criterion = criterion.to(device)\n","\n","############################## 05. NN Model Train Definition #############################\n","\n","def train(model, dataset, optimizer, criterion):\n","\n","    t = time.localtime()\n","    start_time = time.strftime(\"%H:%M:%S\", t)\n","    print(start_time)\n","\n","    epoch_loss = 0\n","    epoch_accuracy = 0\n","\n","    epoch_dataset_length.append(len(dataset))\n","\n","    model.train()\n","\n","    for idx, (sample,label) in enumerate(dataset):\n","       \n","       current_samples = sample\n","       current_labels = label\n","\n","       optimizer.zero_grad()\n","\n","       predicted_labels = model(current_samples).permute(0,2,1)\n","      \n","       loss = criterion(predicted_labels, current_labels)\n","       accuracy = train_accuracy(predicted_labels, current_labels)\n","\n","       loss.backward()\n","       optimizer.step()\n","\n","       epoch_loss += loss.item()\n","       epoch_accuracy += accuracy\n","\n","    return epoch_loss/len(dataset), epoch_accuracy/sum(epoch_dataset_length)\n","\n","##########################################################################################\n","################################ 06. NN Model Eval Definition ############################\n","def evaluate(model, dataset, criterion):\n","    \n","    # start_time = time.time()\n","    # print(start_time)\n","\n","    t = time.localtime()\n","    start_time = time.strftime(\"%H:%M:%S\", t)\n","    print(start_time)\n","\n","    epoch_loss = 0\n","    epoch_accuracy = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for idx, (sample,label) in enumerate(dataset):\n","            current_samples = sample\n","            current_labels = label\n","\n","            predicted_labels = model(current_samples).permute(0,2,1)\n","\n","            loss = criterion(predicted_labels, current_labels)\n","            accuracy = train_accuracy(predicted_labels, current_labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_accuracy += accuracy\n","\n","    return epoch_loss/len(dataset), epoch_accuracy/len(dataset)\n","\n","##########################################################################################\n","\n","################################## 06. NN Model training #####################################\n","#N_EPOCHS = 10\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    print(f\"Epoch #: {epoch}\")\n","    epoch_dataset_length = []\n","    #train the model\n","    train_loss, train_acc = train(model, train_dataset, optimizer, criterion)\n","    \n","    #evaluate the model\n","    valid_loss, valid_acc = evaluate(model, test_dataset, criterion)\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    print(\"-------------------------------------------------------------------\")\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","    print(\"-------------------------------------------------------------------\")\n","\n","# modelpath = \"notebooks\"\n","# torch.save(model.state_dict(), os.path.join(modelpath, \"PennPOSmodel.pth\"))\n","torch.save(model.state_dict(),\"/content/PennPOSmodel.pth\")\n"],"execution_count":145,"outputs":[{"output_type":"stream","text":["====================================================================================================\n","01. Preparing train/test datasets:\n","STEP 01: Look-up Tables...\n","preparing look-up dictionaries\n","preparing train/test/valid datasets\n","done\n","done!\n","Total words in the dictionary: 46349 \n","Total POS tags in the dictionary: 46\n","dictionaries ready!\n","STEP 02: Fetching the dataset...\n","done!\n","STEP 03: Tokens and tags to Numbers...\n","STEP 01: Look-up Tables...\n","preparing look-up dictionaries\n","preparing train/test/valid datasets\n","done\n","done!\n","Total words in the dictionary: 46349 \n","Total POS tags in the dictionary: 46\n","dictionaries ready!\n","STEP 02: Fetching the dataset...\n","done!\n","STEP 03: Tokens and tags to Numbers...\n","datasets ready!\n","====================================================================================================\n","02. Loading Model Parameters:\n","preparing look-up dictionaries\n","preparing train/test/valid datasets\n","done\n","done!\n","Total words in the dictionary: 46349 \n","Total POS tags in the dictionary: 46\n","Size of vocabulary: 46350\tNumber of classes: 46\n","Step 02. builing the model...\n","Done! here is our model:\n","RNNPOSTagger(\n","  (embedding): Embedding(46350, 100, padding_idx=45)\n","  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.1)\n","  (fc): Linear(in_features=64, out_features=46, bias=True)\n","  (activation_fn): LogSoftmax(dim=1)\n",")\n","====================================================================================================\n","Epoch #: 0\n","14:04:59\n","14:05:48\n","-------------------------------------------------------------------\n","\tTrain Loss: 1.372 | Train Acc: 69.51%\n","\t Val. Loss: 1.142 |  Val. Acc: 76.38%\n","-------------------------------------------------------------------\n","Epoch #: 1\n","14:05:50\n","14:06:42\n","-------------------------------------------------------------------\n","\tTrain Loss: 1.079 | Train Acc: 77.15%\n","\t Val. Loss: 1.101 |  Val. Acc: 78.69%\n","-------------------------------------------------------------------\n","Epoch #: 2\n","14:06:44\n","14:07:40\n","-------------------------------------------------------------------\n","\tTrain Loss: 1.023 | Train Acc: 77.77%\n","\t Val. Loss: 1.082 |  Val. Acc: 77.24%\n","-------------------------------------------------------------------\n","Epoch #: 3\n","14:07:42\n","14:08:41\n","-------------------------------------------------------------------\n","\tTrain Loss: 0.995 | Train Acc: 76.92%\n","\t Val. Loss: 1.095 |  Val. Acc: 76.08%\n","-------------------------------------------------------------------\n","Epoch #: 4\n","14:08:43\n","14:09:45\n","-------------------------------------------------------------------\n","\tTrain Loss: 0.984 | Train Acc: 74.23%\n","\t Val. Loss: 1.097 |  Val. Acc: 79.66%\n","-------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QDhX72YXSSz","executionInfo":{"status":"ok","timestamp":1627481422637,"user_tz":-120,"elapsed":4507,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}},"outputId":"b279601b-75a1-48d1-bf2e-fe1f015d7c54"},"source":["############################################################################################\n","################################## 07. Model Predictions #####################################\n","from typing import Tuple, List\n","# from model import RNNPOSTagger\n","# from dataset import WSJDataset, vocabulary\n","# from fetchDataset import myDataset\n","# from prepareDictionary import PennTreeBankDictionary\n","ds2 = PennTreeBankDictionary()\n","valid_ds = myDataset\n","import torch\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import pickle,gzip\n","\n","############################### 01. Look-up dictionaries ####################################\n","word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds2.vocabulary()\n","\n","validation_dataset = DataLoader(dataset=valid_ds(\"PennTreeBankValid\"),batch_size=16,shuffle=False)\n","\n","# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n","# VOCAB_SIZE = len(word_to_idx)+1\n","# EMBED_DIM = 100\n","# HIDDEN_DIM = 64\n","# NUM_LAYERS = 1\n","# NUM_OF_CLASSES = len(pos_to_idx)+1\n","# N_EPOCHS = 5\n","# LEARNING_RATE = 0.005\n","# BATCH_SIZE = 64\n","\n","VOCAB_SIZE = len(word_to_idx)+1\n","EMBED_DIM = 100\n","HIDDEN_DIM = 64\n","NUM_LAYERS = 2\n","NUM_OF_CLASSES = len(pos_to_idx)\n","N_EPOCHS = 20\n","LEARNING_RATE = 0.06\n","BATCH_SIZE = 32\n","\n","print(f\"Our vocab size to the model is therefore: {VOCAB_SIZE}\")\n","################################### 02. NN Model  ########################################\n","\n","print(\"Step 02. builing the model...\")\n","model = RNNPOSTagger(embedding_dimension= EMBED_DIM,\n","                            vocabulary_size=VOCAB_SIZE,\n","                            hidden_dimension=HIDDEN_DIM,\n","                            num_of_layers=NUM_LAYERS,\n","                            dropout=0.1,\n","                            output_dimension=NUM_OF_CLASSES)\n","print(\"----------------------------------------------------------------\")\n","print(\"Done! here is our model:\")\n","print(model)\n","print(\"----------------------------------------------------------------\")\n","\n","################################## 03. load trained model ###############################\n","model.load_state_dict(torch.load(\"/content/PennPOSmodel.pth\"))\n","model.eval()\n","\n","################################## 03. Predictions ###############################\n","print(\"Lets make predictions\")\n","\n","def token_pipeline(x):\n","    \n","    if len(x) < 50:\n","        for i in range(1,60-len(x)):\n","            x.append('PADDING')\n","    return [word_to_idx[tok] for tok in x]\n","\n","def token_reverse_pipeline(x):\n","    return [idx_to_word[idx] for idx in x]\n","\n","def pos_reverse_pipeline(x):\n","    return [idx_to_pos[idx] for idx in x]\n","\n","def pos_pipeline(x):\n","    return [pos_to_idx[pos] for pos in x]\n","##############################################################################################\n","def predict_full_validation_dataset(example_sentence) -> Tuple[List, List]:\n","    sentence_to_tensor = example_sentence.unsqueeze(1).T\n","    with torch.no_grad():\n","        output = model(sentence_to_tensor)\n","        predicted_output = torch.argmax(output, dim=2)\n","        example_predicted_labels = pos_reverse_pipeline(predicted_output.tolist()[0])\n","        example_sentence_words = token_reverse_pipeline(sentence_to_tensor.tolist()[0])\n","\n","    # return example_predicted_labels\n","    return example_sentence_words,example_predicted_labels\n","###############################################################################################\n","def predict_example(example_sentence, example_actual_labels):\n","\n","    \n","\n","    # preprocessing:-\n","    sentence_to_token = token_pipeline(example_sentence)\n","    sentence_to_tensor = torch.tensor(sentence_to_token).unsqueeze(1).T\n","\n","    # predicted labels:-\n","    with torch.no_grad():\n","        output = model(sentence_to_tensor)\n","        predicted_output = torch.argmax(output, dim=2)\n","        print(predicted_output)\n","        #-------------\n","        print(pos_pipeline(example_actual_labels))\n","        # print(predicted_output.tolist()[0][:-1])\n","        print(predicted_output.tolist()[0][:len(example_actual_labels)])\n","\n","        example_predicted_labels = pos_reverse_pipeline(predicted_output.tolist()[0])\n","        print(\"-\"*100)\n","        print(f\"Actual lables:- \\n{example_actual_labels}\")\n","        print(f\"Predicted lables:- \\n{example_predicted_labels[:len(example_actual_labels)]}\")\n","        print(\"-\"*100)\n","    # return example_predicted_labels[:len(example_actual_labels)]\n","###################################################################################################\n","example = [['This', 'time', ',', 'the', 'firms', 'were', 'ready', '.'],\n","            ['We', \"'re\", 'about', 'to', 'see', 'if', 'advertising', 'works', '.']]\n","example_labels = [['DT', 'NN', ',', 'DT', 'NNS', 'VBD', 'JJ', '.'],\n","                ['PRP', 'VBP', 'IN', 'TO', 'VB', 'IN', 'NN', 'VBZ', '.']]\n","\n","predict_example(example_sentence=example[0],example_actual_labels=example_labels[0])\n","print(\"EXAMPLE 2\")\n","predict_example(example_sentence=example[1],example_actual_labels=example_labels[1])\n","\n","##################################################################################################\n","# print(\"Composing the result of first nn network to POS tag the dataset:\")\n","# all_results = []\n","# for idx, (sample, label) in enumerate(validation_dataset):\n","#     for item in sample:\n","#         all_results.append(predict_full_validation_dataset(item))\n","# # print(all_results[:2])\n","# with gzip.open('C:/Users/rahin/projects/WSJ-POS-tagger/data/interim/validation_dataset_pos_tagged.pklz', 'wb') as f:\n","#     pickle.dump(all_results, f)\n","#     f.close()\n","# print(\"done!\")\n","#########################################################################################    \n","\n","\n"],"execution_count":146,"outputs":[{"output_type":"stream","text":["preparing look-up dictionaries\n","preparing train/test/valid datasets\n","done\n","done!\n","Total words in the dictionary: 46349 \n","Total POS tags in the dictionary: 46\n","STEP 01: Look-up Tables...\n","preparing look-up dictionaries\n","preparing train/test/valid datasets\n","done\n","done!\n","Total words in the dictionary: 46349 \n","Total POS tags in the dictionary: 46\n","dictionaries ready!\n","STEP 02: Fetching the dataset...\n","done!\n","STEP 03: Tokens and tags to Numbers...\n","Our vocab size to the model is therefore: 46350\n","Step 02. builing the model...\n","----------------------------------------------------------------\n","Done! here is our model:\n","RNNPOSTagger(\n","  (embedding): Embedding(46350, 100, padding_idx=45)\n","  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.1)\n","  (fc): Linear(in_features=64, out_features=46, bias=True)\n","  (activation_fn): LogSoftmax(dim=1)\n",")\n","----------------------------------------------------------------\n","Lets make predictions\n","tensor([[40, 28, 43, 18, 14,  4, 29, 44, 35, 35, 35, 45, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45]])\n","[18, 28, 43, 18, 14, 4, 29, 44]\n","[40, 28, 43, 18, 14, 4, 29, 44]\n","----------------------------------------------------------------------------------------------------\n","Actual lables:- \n","['DT', 'NN', ',', 'DT', 'NNS', 'VBD', 'JJ', '.']\n","Predicted lables:- \n","['WP$', 'NN', ',', 'DT', 'NNS', 'VBD', 'JJ', '.']\n","----------------------------------------------------------------------------------------------------\n","EXAMPLE 2\n","tensor([[ 7,  4,  9, 27,  5,  6, 28,  2, 44, 35,  0, 35, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n","         45, 45, 45, 45, 45]])\n","[7, 21, 6, 27, 5, 6, 28, 23, 44]\n","[7, 4, 9, 27, 5, 6, 28, 2, 44]\n","----------------------------------------------------------------------------------------------------\n","Actual lables:- \n","['PRP', 'VBP', 'IN', 'TO', 'VB', 'IN', 'NN', 'VBZ', '.']\n","Predicted lables:- \n","['PRP', 'VBD', 'RBS', 'TO', 'VB', 'IN', 'NN', '-LRB-', '.']\n","----------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wqENkahSYr0i","executionInfo":{"status":"ok","timestamp":1627481444814,"user_tz":-120,"elapsed":6416,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}}},"source":["# result comparison\n","all_actual_labels, all_predicted_labels = [],[] \n","\n","def pos_reverse_pipeline(x):\n","    return [idx_to_pos[idx] for idx in x]\n","\n","for idx, (sample,label) in enumerate(validation_dataset):\n","  for sam in sample:\n","    all_predicted_labels.append(predict_full_validation_dataset(sam)[1])\n","  for lab in label:\n","    all_actual_labels.append(pos_reverse_pipeline(lab.tolist()))\n","\n","\n","\n"],"execution_count":147,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D80XOmrHZ00H","executionInfo":{"status":"ok","timestamp":1627481452018,"user_tz":-120,"elapsed":220,"user":{"displayName":"rahichan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinLXc1A9d1Oo-mLwalGyCnLfjeNOOJ0DBTbDreQ=s64","userId":"17245282504808745550"}},"outputId":"6aad13c4-315b-4791-d947-0ece62e06437"},"source":["def train_accuracy(preds, y):\n","\n","    # print(len(preds)) #i get 10 samples\n","    accuracy_of_all_lines = []\n","\n","    \n","    \n","    for pred,act in zip(preds,y):\n","        \n","        counter = 0\n","        \n","        for itemx,itemj in zip(pred,act):\n","          \n","          \n","          if itemx == itemj:\n","              counter = counter+1\n","        accuracy_of_this_line = counter/50\n","\n","        accuracy_of_all_lines.append(accuracy_of_this_line)\n","    \n","    # print(accuracy_of_all_lines)\n","    acc = sum(accuracy_of_all_lines)/len(preds)\n","\n","    return acc*100\n","train_accuracy(all_predicted_labels,all_actual_labels)"],"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["79.32258064516083"]},"metadata":{"tags":[]},"execution_count":149}]}]}